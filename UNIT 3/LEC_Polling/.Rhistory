df <- data.frame(perms,control_avg,treatment_avg,test_statistic)
library(perm)
rm(list = ls())
perms <- chooseMatrix(6,3)
A <- matrix(c(65,68,79.2,60,74,72.6), nrow=6, ncol=1, byrow=TRUE)
treatment_avg <- (1/3)*perms%*%A
control_avg <- (1/3)*(1-perms)%*%A
test_statistic <- abs(treatment_avg-control_avg)
rownumber <- apply(apply(perms, 1,
function(x) (x == c(1, 1, 1, 0, 0, 0))),
2, sum)
rownumber <- (rownumber == 6)
observed_test <- test_statistic[rownumber == TRUE]
#*change information here for students
larger_than_observed <- (test_statistic >= observed_test)
sum(larger_than_observed)
df <- data.frame(perms,control_avg,treatment_avg,test_statistic)
View(df)
phyper(50,400,800,100)
phyper(50,400,800,10)
phyper(50,400,800,100)
phyper(50,400,800,1000)
phyper(50,400,800,200)
phyper(35,400,800,200)
phyper(35,400,800,100)
phyper(35,400,800,200)
phyper(35,400,800,300)
#install.packages("rvest")
#library(rvest)
webpage <- read_html("https://www.cbinsights.com/research-unicorn-companies")
table <- html_nodes(webpage,"table")[[1]]
mytable <- html_table(table)
mytable <- html_table(table,fill=TRUE)
View(mytable)
summary(mytable)
mytable <- html_table(table, fill=TRUE, na.omit(table$node)
mytable <- html_table(table, fill=TRUE, na.omit(table$node))
TT = na.omit(mytable)
summary(TT)
View(TT)
View(mytable)
TT[country == "United States"]
TT$country == "United States"
TT[1:2]
TT[1]
TT$Country
TT[TT$country == "United States"]
QQ = TT[TT$country == "United States"]
View(QQ)
View(TT)
QQ = TT[country == "United States"]
QQ = TT[TT == "United States"]
QQ
QQ = TT[TT == "Uber"]
QQ
QQ = TT[TT == "United State"]
QQ = TT[TT == "United States"]
0.5^2+1.5^2
a
a <- c(2,1,-3,2,1)
a
b <- c(1,0,-1)
b
?convolution
?conv
216*4/(216*4+64*6)
64*6/(216*4+64*6)
A1 = 0.5
A1 <- 0.5
A2 <- 216*4/(216*4+64*6)
B1 <- 0.5
B2 <- 64*6/(216*4+64*6)
log(3)
a = 0.6
b = 0.4
A = 0.6
B = 0.4
QA1 <- 0.5
QA2 <- 216*4/(216*4+64*6)
QB1 <- 0.5
QB2 <- 64*6/(216*4+64*6)
AA = (2*QA1+3*QA2)/(4*QA1+4*QA2)
BB = (2*QB1+3*QB2)/(4*QB1+4*QB2)
FluTrain = read.csv("FluTrain.csv")
setwd("~/R/The Analytics Edge/UNIT 2/Flu Epidemics")
FluTrain = read.csv("FluTrain.csv")
head(FluTrain)
hist(FluTrain$ILI)
# Question ID: 30102
plot(FluTrain$Queries, FluTrain$ILI)
plot(FluTrain$Queries, log(FluTrain$ILI))
View(FluTrain)
head(FluTrain)
end(FluTrain)
tail(FluTrain)
max(FluTrain$ILI)
subset(FluTrain, Week >= 2012)
summary(FluTrain)
which.max(FluTrain$ILI)
index = which.max(FluTrain$ILI)
FluTrain[index]
FluTrain[index,]
View(FluTrain)
index = which.max(FluTrain$Queries)
FluTrain[index,]
View(FluTrain)
lm(ILI ~ Queries, data = FluTrain)
m1
summary(m1)
m1 = lm(ILI ~ Queries, data = FluTrain)
m1
summary(m1)
m2 = lm(log(ILI) ~ Queries, data = FluTrain)
m2
summary(m2)
# Question ID: 30106
cor(FluTrain$Queries, log(FluTrain$ILI))
# Question ID: 30106
c = cor(FluTrain$Queries, log(FluTrain$ILI))
c^2
# Question ID: 30203
FluTrend1 = lm(log(ILI) ~ Queries, data = FluTrain)
FluTest = read.csv("FluTest.csv")
head(FluTest)
tail(FluTest)
PredTest1 = predict(FluTrend1, newdata=FluTest)
PredTest1
PredTest1 = exp(predict(FluTrend1, newdata=FluTest))
PredTest1
View(FluTest)
which(FluTest, Week = 2012-03-11)
which(FluTest, Week = "2012-03-11")
which(FluTest, Week = 2012-03-11 - ,)
which(FluTest, Week = 2012-03-11 - *)
which(FluTest, Week = 2012-03-11 - )
which(FluTest, Week = 2012-03-11 - 2012-03-17)
which(FluTest, Week == 2012-03-11 - 2012-03-17)
which(FluTest, Week = "2012-03-11 - 2012-03-17")
which(FluTest, Week == "2012-03-11 - 2012-03-17")
PredTest1[11]
which(FluTest$Week == "2012-03-11 - 2012-03-17")
which(FluTest$Week == "2012-03-11 - 2012-03-17")
PredTest1[11]
# Question ID: 30204
(FluTest[11] - PredTest1[11])/FluTest[11]
# Question ID: 30204
(FluTest$ILI[11] - PredTest1[11])/FluTest$ILI[11]
# Question ID: 30302
install.packages("zoo").
# Question ID: 30302
install.packages("zoo")
library(zoo)
ILILag2
ILILag2 = lag(zoo(FluTrain$ILI), -2, na.pad=TRUE)
FluTrain$ILILag2 = coredata(ILILag2)
head(FluTrain)
summary(FluTest)
summary(FluTrain)
# Question ID: 30303
FluTrend2 = lm(log(ILI) ~ Queries, data = FluTrain)
FluTrend2
summary(FluTrend2)
head(FluTrain)
# Question ID: 30303
FluTrend2 = lm(log(ILILag2) ~ Queries, data = FluTrain)
FluTrend2
summary(FluTrend2)
# Question ID: 30303
FluTrend2 = lm(log(ILI) ~ Queries + ILILag2, data = FluTrain)
FluTrend2
summary(FluTrend2)
# Question ID: 30401
ILILag2
# Question ID: 30401
ILILag2 = lag(zoo(FluTest$ILI), -2, na.pad=TRUE)
FluTest$ILILag2 = coredata(ILILag2)
summary(FluTest)
# Question ID: 30402
head(FluTrain)
head(FluTest)
# Question ID: 30402
tail(FluTrain)
head(FluTest)
# Question ID: 30304
plot(log(FluTrain$ILILag2), log(FluTrain$ILI))
plot(log(FluTest$ILILag2), log(FluTest$ILI))
# Question ID: 30304
plot(log(FluTrain$ILILag2), log(FluTrain$ILI))
summary(FluTrend2)
# Question ID: 30303
FluTrend2 = lm(log(ILI) ~ Queries + log(ILILag2), data = FluTrain)
FluTrend2
summary(FluTrend2)
# Question ID: 30203
FluTrend1 = lm(log(ILI) ~ Queries, data = FluTrain)
summary(FluTrend1)
# Question ID: 30402
tail(FluTrain)
head(FluTest)
# Question ID: 30404
FluTest$ILILag2[1] = FluTrain$ILI[416]
FluTest$ILILag2[2] = FluTrain$ILI[417]
head(FluTest)
# Question ID: 30405
PredTest2 = predict(FluTrend2, newdata=FluTest)
PredTest2
PredTest2 = exp(predict(FluTrend2, newdata=FluTest))
PredTest2
# Sum of Squared Errors
FluTrend2$residuals
SSE = sum(FluTrend2$residuals^2)
SSE
# Root mean squared error
RMSE = sqrt(SSE/nrow(FluTrain))
RMSE
# Sum of Squared Errors
FluTrend2$residuals
SSE = sum(FluTrend2$residuals^2)
SSE
# Root mean squared error
RMSE = sqrt(SSE/nrow(FluTrain))
RMSE
PredTest2 = exp(predict(FluTrend2, newdata=FluTest))
SSE = sum((PredTest2-FluTest$ILI)^2)
RMSE = sqrt(SSE / nrow(FluTest))
RMSE
sqrt(mean((PredTest2-FluTest$ILI)^2)).
sqrt(mean((PredTest2-FluTest$ILI)^2))
sqrt(mean((PredTest1-FluTest$ILI)^2))
sqrt(mean((PredTest2-FluTest$ILI)^2))
exp(-1)
1/(1+exp(-1))
1/(1+exp(1))
setwd("~/R/The Analytics Edge/UNIT 3")
# Read in dataset
quality = read.csv("quality.csv")
# Look at structure
str(quality)
# Table outcome
table(quality$PoorCare)
# Baseline accuracy
98/131
library(caTools)
# Randomly split data
set.seed(88)
split = sample.split(quality$PoorCare, SplitRatio = 0.75)
split
# Create training and testing sets
qualityTrain = subset(quality, split == TRUE)
qualityTest = subset(quality, split == FALSE)
# Logistic Regression Model
QualityLog = glm(PoorCare ~ OfficeVisits + Narcotics, data=qualityTrain, family=binomial)
summary(QualityLog)
# Make predictions on training set
predictTrain = predict(QualityLog, type="response")
# Analyze predictions
summary(predictTrain)
tapply(predictTrain, qualityTrain$PoorCare, mean)
# Logistic Regression Model
Quality = lm(PoorCare ~ OfficeVisits + Narcotics, data=qualityTrain, family=binomial)
# Logistic Regression Model
Quality = lm(PoorCare ~ OfficeVisits + Narcotics, data=qualityTrain)
summary(Quality)
QualityLog = glm(PoorCare ~ OfficeVisits + Narcotics, data=qualityTrain, family=binomial)
summary(QualityLog)
# Logistic Regression Model
Qualitylm = lm(PoorCare ~ OfficeVisits + Narcotics, data=qualityTrain)
summary(Qualitylm)
head(quality)
# Read in dataset
quality = read.csv("quality.csv")
head(quality)
# Look at structure
str(quality)
# Table outcome
table(quality$PoorCare)
# Baseline accuracy
98/131
# Install and load caTools package
install.packages("caTools")
library(caTools)
# Randomly split data
set.seed(88)
split = sample.split(quality$PoorCare, SplitRatio = 0.75)
split
table(split)
split = sample.split(quality, SplitRatio = 0.75)
split
?split
split = sample.split(quality$PoorCare, SplitRatio = 0.75)
table(split)
# Create training and testing sets
qualityTrain = subset(quality, split == TRUE)
qualityTest = subset(quality, split == FALSE)
# Logistic Regression Model
QualityLog = glm(PoorCare ~ OfficeVisits + Narcotics, data=qualityTrain, family=binomial)
summary(QualityLog)
# Make predictions on training set
predictTrain = predict(QualityLog, type="response")
# Analyze predictions
summary(predictTrain)
tapply(predictTrain, qualityTrain$PoorCare, mean)
# Randomly split data
set.seed(88)
split = sample.split(quality$PoorCare, SplitRatio = 0.75)
# Create training and testing sets
qualityTrain = subset(quality, split == TRUE)
qualityTest = subset(quality, split == FALSE)
# Logistic Regression Model
QualityLog = glm(PoorCare ~ OfficeVisits + Narcotics, data=qualityTrain, family=binomial)
summary(QualityLog)
# Make predictions on training set
predictTrain = predict(QualityLog, type="response")
# Analyze predictions
summary(predictTrain)
tapply(predictTrain, qualityTrain$PoorCare, mean)
# Analyze predictions
summary(predictTrain)
head(quality)
?predict
tapply(predictTrain, qualityTrain$PoorCare, mean)
pisaTrain = read.csv("pisa2009train.csv")
setwd("~/R/The Analytics Edge/UNIT 2/Reading Test Scores")
pisaTrain = read.csv("pisa2009train.csv")
pisaTest = read.csv("pisa2009test.csv")
head(pisaTrain)
tapply(pisaTrain$readingScore, pisaTrain$male, mean)
table(pisaTrain$readingScore, pisaTrain$male)
table(pisaTrain$male, pisaTrain$readingScore)
tapply(pisaTrain$readingScore, pisaTrain$male, mean)
head(pisaTrain)
setwd("~/R/The Analytics Edge/UNIT 3")
summary(QualityLog)
#
m1 = glm(PoorCare ~ StartedOnCombination + ProviderCount, data=qualityTrain, family=binomial)
summary(m1)
head(quality)
# Confusion matrix for threshold of 0.5
table(qualityTrain$PoorCare, predictTrain > 0.5)
# Analyze predictions
summary(predictTrain)
tapply(predictTrain, qualityTrain$PoorCare, mean)
?predict
predictTrain
plot(qualityTrain$OfficeVisits, qualityTrain$PoorCare)
# Analyze predictions
summary(predictTrain)
tapply(predictTrain, qualityTrain$PoorCare, mean)
# Quick Question
m1 = glm(PoorCare ~ StartedOnCombination + ProviderCount, data=qualityTrain, family=binomial)
summary(m1)
# Confusion matrix for threshold of 0.5
table(qualityTrain$PoorCare, predictTrain > 0.5)
# Confusion matrix for threshold of 0.7
table(qualityTrain$PoorCare, predictTrain > 0.7)
# Sensitivity and specificity
8/25
73/74
# Confusion matrix for threshold of 0.2
table(qualityTrain$PoorCare, predictTrain > 0.2)
# Sensitivity and specificity
16/25
54/74
table(predictTrain > 0.2, qualityTrain$PoorCare)
# Confusion matrix for threshold of 0.2
table(qualityTrain$PoorCare, predictTrain > 0.2)
# Confusion matrix for threshold of 0.2
table(qualityTrain$PoorCare, predictTrain > 0.2)
library(ROCR)
# Install and load ROCR package
install.packages("ROCR")
library(ROCR)
# Prediction function
ROCRpred = prediction(predictTrain, qualityTrain$PoorCare)
# Performance function
ROCRperf = performance(ROCRpred, "tpr", "fpr")
# Plot ROC curve
plot(ROCRperf)
# Add colors
plot(ROCRperf, colorize=TRUE)
# Add threshold labels
plot(ROCRperf, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
plot(ROCRperf, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1))
plot(ROCRperf, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
# Logistic Regression Model
QualityLog = glm(PoorCare ~ OfficeVisits + Narcotics, data=qualityTrain, family=binomial)
summary(QualityLog)
# AUC
QualityLog = glm(PoorCare ~ OfficeVisits + Narcotics, data=qualityTrain, family=binomial)
summary(QualityLog)
predictTest = predict(QualityLog, type="response", newdata=qualityTest)
ROCRpredTest = prediction(predictTest, qualityTest$PoorCare)
auc = as.numeric(performance(ROCRpredTest, "auc")@y.values)
auc
setwd("~/R/The Analytics Edge/UNIT 3/Framingham")
# Read in the dataset
framingham = read.csv("framingham.csv")
# Look at structure
str(framingham)
# Load the library caTools
library(caTools)
# Randomly split the data into training and testing sets
set.seed(1000)
split = sample.split(framingham$TenYearCHD, SplitRatio = 0.65)
# Split up the data using subset
train = subset(framingham, split==TRUE)
test = subset(framingham, split==FALSE)
# Logistic Regression Model
framinghamLog = glm(TenYearCHD ~ ., data = train, family=binomial)
summary(framinghamLog)
# Predictions on the test set
predictTest = predict(framinghamLog, type="response", newdata=test)
# Confusion matrix with threshold of 0.5
table(test$TenYearCHD, predictTest > 0.5)
# Accuracy
(1069+11)/(1069+6+187+11)
# Baseline accuracy
(1069+6)/(1069+6+187+11)
# Test set AUC
library(ROCR)
ROCRpred = prediction(predictTest, test$TenYearCHD)
as.numeric(performance(ROCRpred, "auc")@y.values)
11/198
1069/1075
summary(framinghamLog)
step(framinghamLog)
?step
m2 = step(framinghamLog)
summary(m2)
# Logistic Regression Model
framinghamLog = glm(TenYearCHD ~ ., data = train, family=binomial)
summary(framinghamLog)
#
m2 = glm(TenYearCHD ~ male + age + cigsPerDay + prevalentStroke + prevalentHyp + totChol + sysBP + glucose, data = train, family=binomial)
summary(m2)
setwd("~/R/The Analytics Edge/UNIT 3/Polling")
# Read in data
polling = read.csv("PollingData.csv")
str(polling)
table(polling$Year)
summary(polling)
library(mice)
# Install and load mice package
install.packages("mice")
library(mice)
summary(polling)
# Multiple imputation
simple = polling[c("Rasmussen", "SurveyUSA", "PropR", "DiffCount")]
summary(simple)
set.seed(144)
imputed = complete(mice(simple))
summary(imputed)
summary(polling)
summary(simple)
summary(imputed)
polling$Rasmussen = imputed$Rasmussen
polling$SurveyUSA = imputed$SurveyUSA
summary(polling)
# Subset data into training set and test set
Train = subset(polling, Year == 2004 | Year == 2008)
Test = subset(polling, Year == 2012)
# Smart Baseline
table(Train$Republican)
sign(20)
sign(-10)
sign(0)
table(sign(Train$Rasmussen))
table(Train$Republican, sign(Train$Rasmussen))
sign(0)
sign(-10)
sign(20)
table(sign(Train$Rasmussen))
table(Train$Republican, sign(Train$Rasmussen))
# Multicollinearity
cor(Train)
# Multicollinearity
cor(Train)
str(Train)
cor(Train[c("Rasmussen", "SurveyUSA", "PropR", "DiffCount", "Republican")])
Train
# Logistic Regression Model
mod1 = glm(Republican ~ PropR, data=Train, family="binomial")
summary(mod1)
# Training set predictions
pred1 = predict(mod1, type="response")
table(Train$Republican, pred1 >= 0.5)
# Two-variable model
mod2 = glm(Republican ~ SurveyUSA + DiffCount, data=Train, family="binomial")
pred2 = predict(mod2, type="response")
pred2 = predict(mod2, type="response")
table(Train$Republican, pred2 >= 0.5)
summary(mod2)
summary(mod1)
summary(mod2)
cor(Train[c("Rasmussen", "SurveyUSA", "PropR", "DiffCount", "Republican")])
step(mod2)
step(mod1)
table(Train$Republican, pred2 >= 0.5)
summary(mod2)
plot(Train$Republican, Train$SurveyUSA)
# Smart baseline accuracy
table(Test$Republican, sign(Test$Rasmussen))
# Test set predictions
TestPrediction = predict(mod2, newdata=Test, type="response")
table(Test$Republican, TestPrediction >= 0.5)
# Analyze mistake
subset(Test, TestPrediction >= 0.5 & Republican == 0)
# Smart baseline accuracy
table(Test$Republican, sign(Test$Rasmussen))
table(Train$Republican, pred2 >= 0.5)
table(Train$Republican, sign(Train$Rasmussen))
table(Test$Republican, TestPrediction >= 0.5)
