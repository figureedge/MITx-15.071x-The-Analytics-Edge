mean(thetain)
#preliminaries
rm(list = ls())
setwd("")
#Real value of theta
theta <- 5
#sample size
n <- 25
#Generating 1000 samples of size n
simul <- 1000
sample <- matrix(runif(simul*n, max = theta),
nrow = n)
thetahat <- (n+1)/n*apply(sample, 2, max)
ll <- thetahat/(0.95^(1/n)*(n+1)/n)
ul <- thetahat/(0.05^(1/n)*(n+1)/n)
thetain <- (theta>=ll & theta<=ul)
mean(thetain)
#preliminaries
rm(list = ls())
setwd("")
#Real value of theta
theta <- 5
#sample size
n <- 25
#Generating 1000 samples of size n
simul <- 1000
sample <- matrix(runif(simul*n, max = theta),
nrow = n)
thetahat <- (n+1)/n*apply(sample, 2, max)
ll <- thetahat/(0.95^(1/n)*(n+1)/n)
ul <- thetahat/(0.05^(1/n)*(n+1)/n)
thetain <- (theta>=ll & theta<=ul)
mean(thetain)
#preliminaries
rm(list = ls())
setwd("")
#Real value of theta
theta <- 5
#sample size
n <- 25
#Generating 1000 samples of size n
simul <- 1000
sample <- matrix(runif(simul*n, max = theta),
nrow = n)
thetahat <- (n+1)/n*apply(sample, 1, max)
ll <- thetahat/(0.95^(1/n)*(n+1)/n)
ul <- thetahat/(0.05^(1/n)*(n+1)/n)
thetain <- (theta>=ll & theta<=ul)
mean(thetain)
#preliminaries
rm(list = ls())
setwd("")
#Real value of theta
theta <- 5
#sample size
n <- 25
#Generating 1000 samples of size n
simul <- 1000
sample <- matrix(runif(simul*n, max = theta),
nrow = n)
thetahat <- (n+1)/n*apply(sample, 1, max)
ll <- thetahat/(0.95^(1/n)*(n+1)/n)
ul <- thetahat/(0.05^(1/n)*(n+1)/n)
thetain <- (theta>=ll & theta<=ul)
mean(thetain)
#preliminaries
rm(list = ls())
setwd("")
#Real value of theta
theta <- 5
#sample size
n <- 25
#Generating 1000 samples of size n
simul <- 1000
sample <- matrix(runif(simul*n, max = theta),
nrow = n)
thetahat <- (n+1)/n*apply(sample, 1, max)
ll <- thetahat/(0.95^(1/n)*(n+1)/n)
ul <- thetahat/(0.05^(1/n)*(n+1)/n)
thetain <- (theta>=ll & theta<=ul)
mean(thetain)
#preliminaries
rm(list = ls())
setwd("")
#Real value of theta
theta <- 5
#sample size
n <- 25
#Generating 1000 samples of size n
simul <- 1000
sample <- matrix(runif(simul*n, max = theta),
nrow = n)
thetahat <- (n+1)/n*apply(sample, 1, max)
ll <- thetahat/(0.95^(1/n)*(n+1)/n)
ul <- thetahat/(0.05^(1/n)*(n+1)/n)
thetain <- (theta>=ll & theta<=ul)
mean(thetain)
#preliminaries
rm(list = ls())
setwd("")
#Real value of theta
theta <- 5
#sample size
n <- 25
#Generating 1000 samples of size n
simul <- 1000
sample <- matrix(runif(simul*n, max = theta),
nrow = n)
thetahat <- (n+1)/n*apply(sample, 1, max)
ll <- thetahat/(0.95^(1/n)*(n+1)/n)
ul <- thetahat/(0.05^(1/n)*(n+1)/n)
thetain <- (theta>=ll & theta<=ul)
mean(thetain)
#preliminaries
rm(list = ls())
setwd("")
#Real value of theta
theta <- 5
#sample size
n <- 25
#Generating 1000 samples of size n
simul <- 1000
sample <- matrix(runif(simul*n, max = theta),
nrow = n)
thetahat <- (n+1)/n*apply(sample, 1, max)
ll <- thetahat/(0.95^(1/n)*(n+1)/n)
ul <- thetahat/(0.05^(1/n)*(n+1)/n)
thetain <- (theta>=ll & theta<=ul)
mean(thetain)
?apply
sample
#preliminaries
rm(list = ls())
setwd("")
#Real value of theta
theta <- 5
#sample size
n <- 25
#Generating 1000 samples of size n
simul <- 1000
sample <- matrix(runif(simul*n, max = theta),
nrow = n)
thetahat <- (n+1)/n*apply(sample, 2, max)
ll <- thetahat/(0.95^(1/n)*(n+1)/n)
ul <- thetahat/(0.05^(1/n)*(n+1)/n)
thetain <- (theta>=ll & theta<=ul)
mean(thetain)
head(cars)  # display the first 6 observations
cars
install.packages("rvest")
library(rvest)
webpage <- read_html("https://www.cbinsights.com/research-unicorn-companies")
table <- html_nodes(webpage,"table")[[1]]
View(webpage)
View(table)
View(sample)
webpage <- read_html("https://www.cbinsights.com/research-unicorn-companies")
table <- html_nodes(webpage,"table")[[1]]
mytable <- html_table(table)
drug = c(6.1,7,8.2,7.6,6.5,7.8,6.9,6.7,7.4,5.8)
placebo = c(5.2,7.9,3.9,4.7,5.3,4.8,4.2,6.1,3.8,6.3)
difference = drug-placebo
summary(difference)
var(difference)
?var
mean(difference)
drug = c(6.1,7,8.2,7.6,6.5,7.8,6.9,6.7,7.4,5.8)
placebo = c(5.2,7.9,3.9,4.7,5.3,4.8,4.2,6.1,3.8,6.3)
difference = drug-placebo
summary(difference)
std = sqrt(var(difference))
mean(difference)
sqrt(length(difference))*mean(difference)/std
y = mean(difference)
sqrt(length(difference))*mean(difference)/std
t.test(difference,mu=y)
?t.test
t.test(difference,mu=y,alternative = "greater")
mean(difference)
sqrt(length(difference))*mean(difference)/std
ptnorm(0.05)
ptnorm(t,y,sd)
?ptnorm
pnorm(t,y,sd)
sd = sqrt(var(difference))
y = mean(difference)
t = sqrt(length(difference))*mean(difference)/sd
pnorm(t,y,sd)
sd
y
t
ptnorm(t,y,sd)
sd = sqrt(var(difference))
m = mean(difference)
t = sqrt(length(difference))*mean(difference)/sd
sd
m
t
sd = sqrt(var(difference))
m = mean(difference)
n = length(difference)
t = sqrt(n)*m/sd
t
st
sd
t.test(difference,mu=m,alternative = "greater")
t.test(difference,mu=m)
t.test(difference,mu=0)
t.test(difference,mu=0,alternative = "greater")
sd
sd^2
perms <- chooseMatrix(8,4)
A <- matrix(c(85, 99, 100, 76, 26, 45, 97, 72), nrow=8, ncol=1, byrow=TRUE)
treatment_avg <- (1/4)*perms%*%A
control_avg <- (1/4)*(1-perms)%*%A
test_statistic <- abs(treatment_avg-control_avg)
rownumber <- apply(apply(perms, 1,
function(x) (x == c(1, 1, 1, 1, 0, 0, 0, 0))),
2, sum)
rownumber <- (rownumber == 8)
observed_test <- test_statistic[rownumber == TRUE]
#*change information here for students
larger_than_observed <- (test_statistic >= observed_test)
sum(larger_than_observed)
df <- data.frame(perms,control_avg,treatment_avg,test_statistic)
library(perm)
rm(list = ls())
perms <- chooseMatrix(8,4)
A <- matrix(c(85, 99, 100, 76, 26, 45, 97, 72), nrow=8, ncol=1, byrow=TRUE)
treatment_avg <- (1/4)*perms%*%A
control_avg <- (1/4)*(1-perms)%*%A
test_statistic <- abs(treatment_avg-control_avg)
rownumber <- apply(apply(perms, 1,
function(x) (x == c(1, 1, 1, 1, 0, 0, 0, 0))),
2, sum)
rownumber <- (rownumber == 8)
observed_test <- test_statistic[rownumber == TRUE]
#*change information here for students
larger_than_observed <- (test_statistic >= observed_test)
sum(larger_than_observed)
df <- data.frame(perms,control_avg,treatment_avg,test_statistic)
View(A)
View(treatment_avg)
View(perms)
View(df)
sum(larger_than_observed)
rownumber
rownumber <- apply(apply(perms, 1,
function(x) (x == c(1, 1, 1, 1, 0, 0, 0, 0))),
2, sum)
rownumber
library(perm)
rm(list = ls())
perms <- chooseMatrix(6,3)
A <- matrix(c(65,68,79.2,60,74,72.6), nrow=6, ncol=1, byrow=TRUE)
treatment_avg <- (1/3)*perms%*%A
control_avg <- (1/3)*(1-perms)%*%A
test_statistic <- abs(treatment_avg-control_avg)
rownumber <- apply(apply(perms, 1,
function(x) (x == c(1, 1, 1, 0, 0, 0))),
2, sum)
rownumber <- (rownumber == 6)
observed_test <- test_statistic[rownumber == TRUE]
#*change information here for students
larger_than_observed <- (test_statistic >= observed_test)
sum(larger_than_observed)
df <- data.frame(perms,control_avg,treatment_avg,test_statistic)
View(treatment_avg)
View(perms)
60+74+72
65+68+79
206/3
212/3
library(perm)
rm(list = ls())
perms <- chooseMatrix(6,3)
A <- matrix(c(65,68,79.2,60,74,72.6), nrow=6, ncol=1, byrow=TRUE)
treatment_avg <- (1/3)*perms%*%A
control_avg <- (1/3)*(1-perms)%*%A
test_statistic <- abs(treatment_avg-control_avg)
rownumber <- apply(apply(perms, 1,
function(x) (x == c(0, 0, 0,1,1,1))),
2, sum)
rownumber <- (rownumber == 6)
observed_test <- test_statistic[rownumber == TRUE]
#*change information here for students
larger_than_observed <- (test_statistic >= observed_test)
sum(larger_than_observed)
df <- data.frame(perms,control_avg,treatment_avg,test_statistic)
library(perm)
rm(list = ls())
perms <- chooseMatrix(6,3)
A <- matrix(c(65,68,79.2,60,74,72.6), nrow=6, ncol=1, byrow=TRUE)
treatment_avg <- (1/3)*perms%*%A
control_avg <- (1/3)*(1-perms)%*%A
test_statistic <- abs(treatment_avg-control_avg)
rownumber <- apply(apply(perms, 1,
function(x) (x == c(1, 1, 1, 0, 0, 0))),
1, sum)
rownumber <- (rownumber == 6)
observed_test <- test_statistic[rownumber == TRUE]
#*change information here for students
larger_than_observed <- (test_statistic >= observed_test)
sum(larger_than_observed)
df <- data.frame(perms,control_avg,treatment_avg,test_statistic)
library(perm)
rm(list = ls())
perms <- chooseMatrix(6,3)
A <- matrix(c(65,68,79.2,60,74,72.6), nrow=6, ncol=1, byrow=TRUE)
treatment_avg <- (1/3)*perms%*%A
control_avg <- (1/3)*(1-perms)%*%A
test_statistic <- abs(treatment_avg-control_avg)
rownumber <- apply(apply(perms, 1,
function(x) (x == c(1, 1, 1, 0, 0, 0))),
2, sum)
rownumber <- (rownumber == 6)
observed_test <- test_statistic[rownumber == TRUE]
#*change information here for students
larger_than_observed <- (test_statistic >= observed_test)
sum(larger_than_observed)
df <- data.frame(perms,control_avg,treatment_avg,test_statistic)
View(df)
phyper(50,400,800,100)
phyper(50,400,800,10)
phyper(50,400,800,100)
phyper(50,400,800,1000)
phyper(50,400,800,200)
phyper(35,400,800,200)
phyper(35,400,800,100)
phyper(35,400,800,200)
phyper(35,400,800,300)
#install.packages("rvest")
#library(rvest)
webpage <- read_html("https://www.cbinsights.com/research-unicorn-companies")
table <- html_nodes(webpage,"table")[[1]]
mytable <- html_table(table)
mytable <- html_table(table,fill=TRUE)
View(mytable)
summary(mytable)
mytable <- html_table(table, fill=TRUE, na.omit(table$node)
mytable <- html_table(table, fill=TRUE, na.omit(table$node))
TT = na.omit(mytable)
summary(TT)
View(TT)
View(mytable)
TT[country == "United States"]
TT$country == "United States"
TT[1:2]
TT[1]
TT$Country
TT[TT$country == "United States"]
QQ = TT[TT$country == "United States"]
View(QQ)
View(TT)
QQ = TT[country == "United States"]
QQ = TT[TT == "United States"]
QQ
QQ = TT[TT == "Uber"]
QQ
QQ = TT[TT == "United State"]
QQ = TT[TT == "United States"]
0.5^2+1.5^2
a
a <- c(2,1,-3,2,1)
a
b <- c(1,0,-1)
b
?convolution
?conv
216*4/(216*4+64*6)
64*6/(216*4+64*6)
A1 = 0.5
A1 <- 0.5
A2 <- 216*4/(216*4+64*6)
B1 <- 0.5
B2 <- 64*6/(216*4+64*6)
log(3)
a = 0.6
b = 0.4
A = 0.6
B = 0.4
QA1 <- 0.5
QA2 <- 216*4/(216*4+64*6)
QB1 <- 0.5
QB2 <- 64*6/(216*4+64*6)
AA = (2*QA1+3*QA2)/(4*QA1+4*QA2)
BB = (2*QB1+3*QB2)/(4*QB1+4*QB2)
setwd("~/R/The Analytics Edge/UNIT 3/HW_Popularity of Music Records")
songs = read.csv("songs.csv")
head(songs)
str(songs)
# 2.1
SongsTrain = subset(songs, year != 2010)
SongsTest = subset(songs, year == 2010)
nonvars = c("year", "songtitle", "artistname", "songID", "artistID")
SongsTrain = SongsTrain[ , !(names(SongsTrain) %in% nonvars) ]
SongsTest = SongsTest[ , !(names(SongsTest) %in% nonvars) ]
head(SongsTrain)
SongsLog1 = glm(Top10 ~ ., data = SongsTrain, family = binomial)
summary(SongsLog1)
head(MJ)
MJ = subset(songs, artistname == "Michael Jackson")
head(MJ)
str(MJ)
summary(SongsLog1)
summary(SongsLog1)
# 3.1
cor(SongsTrain$loudness, SongsTrain$energy)
step(SongsLog1)
# 3.2
SongsLog2 = glm(Top10 ~ . - loudness, data=SongsTrain, family=binomial)
summary(SongsLog2)
SongsLog2 = glm(Top10 ~ . - energy, data = SongsTrain, family = binomial)
summary(SongsLog2)
# 3.3
SongsLog3 = glm(Top10 ~ . - energy, data = SongsTrain, family = binomial)
summary(SongsLog3)
# 3.2
SongsLog2 = glm(Top10 ~ . - loudness, data = SongsTrain, family = binomial)
summary(SongsLog2)
# 3.2
SongsLog2 = glm(Top10 ~ . - loudness, data = SongsTrain, family = binomial)
summary(SongsLog2)
SongsLog1 = glm(Top10 ~ ., data = SongsTrain, family = binomial)
summary(SongsLog1)
# 3.2
SongsLog2 = glm(Top10 ~ . - loudness, data = SongsTrain, family = binomial)
summary(SongsLog2)
# 3.3
SongsLog3 = glm(Top10 ~ . - energy, data = SongsTrain, family = binomial)
summary(SongsLog3)
setwd("~/R/The Analytics Edge/UNIT 3/LEC_Framingham")
# Read in the dataset
framingham = read.csv("framingham.csv")
# Randomly split the data into training and testing sets
set.seed(1000)
split = sample.split(framingham$TenYearCHD, SplitRatio = 0.65)
# Load the library caTools
library(caTools)
# Randomly split the data into training and testing sets
set.seed(1000)
split = sample.split(framingham$TenYearCHD, SplitRatio = 0.65)
# Split up the data using subset
train = subset(framingham, split==TRUE)
test = subset(framingham, split==FALSE)
# Logistic Regression Model
framinghamLog = glm(TenYearCHD ~ ., data = train, family=binomial)
summary(framinghamLog)
# Predictions on the test set
predictTest = predict(framinghamLog, type="response", newdata=test)
# Confusion matrix with threshold of 0.5
table(test$TenYearCHD, predictTest > 0.5)
# Predictions on the test set
predictTest = predict(SongsLog3, type="response", newdata=SongsTest)
# Confusion matrix with threshold of 0.45
table(SongsTest$Top10, predictTest > 0.45)
# Accuracy
(309+19)/(309+19+5+40)
# Confusion matrix with threshold of 0.45
table(SongsTest$Top10, predictTest >= 0.45)
# 4.2 baseline model
predictTest = predict(SongsLog1, type="response", newdata=SongsTest)
table(SongsTest$Top10, predictTest >= 0.45)
(309+15)/(309+15+5+44)
head(MJ)
# 4.2 baseline model
predictTest = predict(SongsLog3, type="response", newdata=subset(SongsTest, Top10 == 0))
table(SongsTest$Top10, predictTest >= 0.45)
out = subset(SongsTest, Top10 == 0)
out = subset(SongsTest, Top10 == 0)
predictTest = predict(SongsLog3, type="response", newdata=out)
table(out$Top10, predictTest >= 0.45)
309/(309+5)
out = subset(SongsTest, Top10 == 0)
SongsLog3 = glm(Top10 ~ . - energy, data = out, family = binomial)
# Confusion matrix with threshold of 0.45
table(SongsTest$Top10, predictTest >= 0.45)
# 3.3
SongsLog3 = glm(Top10 ~ . - energy, data = SongsTrain, family = binomial)
summary(SongsLog3)
# Predictions on the test set
predictTest = predict(SongsLog3, type="response", newdata=SongsTest)
# Confusion matrix with threshold of 0.45
table(SongsTest$Top10, predictTest >= 0.45)
# 4.4
19/(40+19)
309/(309+5)
# Predictions on the test set
predictTest = predict(SongsLog3, type="response", newdata=SongsTest)
# Confusion matrix with threshold of 0.45
table(SongsTest$Top10, predictTest >= 0.45)
# Accuracy
(309+19)/(309+19+5+40)
# 4.2 baseline model
predictTest = predict(SongsLog1, type="response", newdata=SongsTest)
table(SongsTest$Top10, predictTest >= 0.45)
(309+15)/(309+15+5+44)
predictTest = predict(SongsLog2, type="response", newdata=SongsTest)
table(SongsTest$Top10, predictTest >= 0.45)
(311+15)/(311+15+3+44)
# 3.3
SongsLog3 = glm(Top10 ~ . - energy, data = SongsTrain, family = binomial)
summary(SongsLog3)
# Predictions on the test set
predictTest = predict(SongsLog3, type="response", newdata=SongsTest)
# Confusion matrix with threshold of 0.45
table(SongsTest$Top10, predictTest >= 0.45)
# Accuracy
(309+19)/(309+19+5+40)
# 4.2 baseline model
table(SongsTest$Top10)
314/(314+59)
